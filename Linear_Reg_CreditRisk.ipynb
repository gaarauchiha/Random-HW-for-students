{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3bf1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file\n",
      "file shape (55515, 9)\n",
      "date encoding\n",
      "False False\n",
      "remove null datapoints to see if it helps...\n",
      "eliminating small count categories\n",
      "replacing labels\n",
      "debt_consolidation    26255\n",
      "other                  9130\n",
      "credit_card            6020\n",
      "home_improvement       3159\n",
      "major_purchase         2145\n",
      "car                    2126\n",
      "moving                 1594\n",
      "medical                1443\n",
      "small_business         1002\n",
      "vacation                883\n",
      "house                   869\n",
      "Name: purpose, dtype: int64\n",
      "hot encoding\n",
      "data columns AFTER hot encoding       Index(['dti', 'emp_length', 'issue_d', 'loan_amnt', 'rejected', 'Unnamed: 7',\n",
      "       'Unnamed: 8', 'Unnamed: 9', 'purpose_car', 'purpose_credit_card',\n",
      "       'purpose_debt_consolidation', 'purpose_home_improvement',\n",
      "       'purpose_house', 'purpose_major_purchase', 'purpose_medical',\n",
      "       'purpose_moving', 'purpose_other', 'purpose_small_business',\n",
      "       'purpose_vacation'],\n",
      "      dtype='object')\n",
      "Number of loans in the partition:    54626\n",
      "Number of loans in the full dataset: 54626\n",
      "fitting\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS  N551JW\\AppData\\Local\\Temp\\ipykernel_4884\\3242789263.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train.drop('issue_d', axis=1, inplace=True)\n",
      "C:\\Users\\ASUS  N551JW\\AppData\\Local\\Temp\\ipykernel_4884\\3242789263.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test.drop('issue_d', axis=1, inplace=True)\n",
      "C:\\Users\\ASUS  N551JW\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS  N551JW\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters        {'cv': 5, 'error_score': nan, 'estimator__memory': None, 'estimator__steps': [('imputer', SimpleImputer(copy=False)), ('scaler', StandardScaler(copy=False)), ('model', SGDClassifier(class_weight='balanced', loss='log', max_iter=10, n_jobs=10,\n",
      "              random_state=1, warm_start=True))], 'estimator__verbose': False, 'estimator__imputer': SimpleImputer(copy=False), 'estimator__scaler': StandardScaler(copy=False), 'estimator__model': SGDClassifier(class_weight='balanced', loss='log', max_iter=10, n_jobs=10,\n",
      "              random_state=1, warm_start=True), 'estimator__imputer__add_indicator': False, 'estimator__imputer__copy': False, 'estimator__imputer__fill_value': None, 'estimator__imputer__keep_empty_features': False, 'estimator__imputer__missing_values': nan, 'estimator__imputer__strategy': 'mean', 'estimator__imputer__verbose': 'deprecated', 'estimator__scaler__copy': False, 'estimator__scaler__with_mean': True, 'estimator__scaler__with_std': True, 'estimator__model__alpha': 0.0001, 'estimator__model__average': False, 'estimator__model__class_weight': 'balanced', 'estimator__model__early_stopping': False, 'estimator__model__epsilon': 0.1, 'estimator__model__eta0': 0.0, 'estimator__model__fit_intercept': True, 'estimator__model__l1_ratio': 0.15, 'estimator__model__learning_rate': 'optimal', 'estimator__model__loss': 'log', 'estimator__model__max_iter': 10, 'estimator__model__n_iter_no_change': 5, 'estimator__model__n_jobs': 10, 'estimator__model__penalty': 'l2', 'estimator__model__power_t': 0.5, 'estimator__model__random_state': 1, 'estimator__model__shuffle': True, 'estimator__model__tol': 0.001, 'estimator__model__validation_fraction': 0.1, 'estimator__model__verbose': 0, 'estimator__model__warm_start': True, 'estimator': Pipeline(steps=[('imputer', SimpleImputer(copy=False)),\n",
      "                ('scaler', StandardScaler(copy=False)),\n",
      "                ('model',\n",
      "                 SGDClassifier(class_weight='balanced', loss='log', max_iter=10,\n",
      "                               n_jobs=10, random_state=1, warm_start=True))]), 'n_jobs': 5, 'param_grid': {'model__alpha': [0.001, 0.01, 10], 'model__penalty': ['l1', 'l2']}, 'pre_dispatch': 3, 'refit': True, 'return_train_score': False, 'scoring': 'roc_auc', 'verbose': 5}\n",
      "{'model__alpha': 0.001, 'model__penalty': 'l2'} 0.5132230733433962\n",
      "LOOK FOR DISCREPANCIES HERE...\n",
      "0.48833311205523106 0.6635687732342007 0.32142857142857145\n"
     ]
    }
   ],
   "source": [
    "'''import the required packages and read the file.'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print('reading file')\n",
    "\n",
    "data = pd.read_csv('input_file_1_subs.csv', sep = ',', index_col=0)\n",
    "\n",
    "print('file shape', data.shape)\n",
    "\n",
    "'''parse column to date format'''\n",
    "\n",
    "print('date encoding')\n",
    "\n",
    "data['issue_d'] = pd.to_datetime(data['issue_d'])\n",
    "\n",
    "'''check for and remove datapoints with null values.'''\n",
    "\n",
    "print(data['issue_d'].isnull().any(), data['purpose'].isnull().any())\n",
    "\n",
    "print('remove null datapoints to see if it helps...')\n",
    "\n",
    "data = data.loc[data['purpose'].isnull() == False]\n",
    "\n",
    "'''eliminate purpose categories with low count.'''\n",
    "\n",
    "print('eliminating small count categories')\n",
    "\n",
    "threshold = 190\n",
    "\n",
    "counts = data['purpose'].value_counts()\n",
    "\n",
    "keep_list = counts[counts > threshold].index\n",
    "\n",
    "data = data[data['purpose'].isin(keep_list)]\n",
    "\n",
    "'''replace the existing labels so that they can be called easily from pandas and TensorFlow'''\n",
    "\n",
    "print('replacing labels')\n",
    "\n",
    "to_replace = {\n",
    "    'Debt consolidation': 'debt_consolidation',\n",
    "    'Home improvement': 'home_improvement',\n",
    "    'Credit card refinancing': 'credit_card',\n",
    "    'Other': 'other',\n",
    "    'Vacation': 'vacation',\n",
    "    'Medical expenses': 'medical',\n",
    "    'Car financing': 'car',\n",
    "    'Major purchase': 'major_purchase',\n",
    "    'Moving and relocation': 'moving',\n",
    "    'Home buying': 'house'\n",
    "}\n",
    "\n",
    "data['purpose'] = data['purpose'].replace(to_replace)\n",
    "\n",
    "print(data['purpose'].value_counts())\n",
    "\n",
    "'''Create one-hot encoded dummy columns for categorical variables.'''\n",
    "\n",
    "print('hot encoding')\n",
    "\n",
    "data = pd.get_dummies(data, columns=['purpose'], drop_first=False)\n",
    "\n",
    "print('data columns AFTER hot encoding      ', data.columns)\n",
    "\n",
    "'''split training and test data by date quantile.'''\n",
    "\n",
    "data_train = data.loc[data['issue_d'] < data['issue_d'].quantile(0.9)]\n",
    "data_test = data.loc[data['issue_d'] >= data['issue_d'].quantile(0.9)]\n",
    "\n",
    "print('Number of loans in the partition:   ', data_train.shape[0] + data_test.shape[0])\n",
    "print('Number of loans in the full dataset:', data.shape[0])\n",
    "\n",
    "'''Drop the date column as not needed for the model.'''\n",
    "\n",
    "data_train.drop('issue_d', axis=1, inplace=True)\n",
    "data_test.drop('issue_d', axis=1, inplace=True)\n",
    "\n",
    "'''Split features and labels'''\n",
    "\n",
    "y_train = data_train['rejected']\n",
    "y_test = data_test['rejected']\n",
    "X_train = data_train.drop('rejected', axis=1)\n",
    "X_test = data_test.drop('rejected', axis=1)\n",
    "\n",
    "# Check if y_train contains more than one unique class label\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    '''Build a pipeline for preprocessing and training'''\n",
    "    pipeline_sgdlogreg = Pipeline([\n",
    "        ('imputer',  SimpleImputer(copy=False)), # Mean imputation by default\n",
    "        ('scaler', StandardScaler(copy=False)),\n",
    "        ('model', SGDClassifier(\n",
    "            class_weight='balanced',\n",
    "            loss='log',\n",
    "            max_iter=10,\n",
    "            tol = 1e-3,\n",
    "            random_state=1,\n",
    "            n_jobs=10,\n",
    "            warm_start=True\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    param_grid_sgdlogreg = {\n",
    "        'model__alpha': [10**-3, 10**-2, 10**1],\n",
    "        'model__penalty': ['l1', 'l2']\n",
    "    }\n",
    "\n",
    "    '''Set up a grid search.'''\n",
    "    grid_sgdlogreg = GridSearchCV(\n",
    "        estimator=pipeline_sgdlogreg,\n",
    "        param_grid=param_grid_sgdlogreg,\n",
    "        scoring='roc_auc',\n",
    "        pre_dispatch=3,\n",
    "        n_jobs=5,\n",
    "        cv=5,\n",
    "        verbose=5,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    '''Fit the model.'''\n",
    "    print('fitting')\n",
    "    grid_sgdlogreg.fit(X_train, y_train)\n",
    "\n",
    "    '''Print model parameters, best parameters and best score.'''\n",
    "    print('parameters       ', grid_sgdlogreg.get_params())\n",
    "    print(grid_sgdlogreg.best_params_, grid_sgdlogreg.best_score_)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, recall_score\n",
    "\n",
    "    '''Make predictions on test dataset.'''\n",
    "    y_score = grid_sgdlogreg.predict_proba(X_test)[:,1]\n",
    "    y_score_flag = [int(round(i)) for i in y_score]\n",
    "\n",
    "    print('LOOK FOR DISCREPANCIES HERE...')\n",
    "    print(roc_auc_score(y_test, y_score), recall_score(y_test, y_score_flag, pos_label=1), recall_score(y_test, y_score_flag, pos_label=0))\n",
    "\n",
    "    y_score_flag = grid_sgdlogreg.predict(X_test)\n",
    "else:\n",
    "    print(\"y_train contains only one unique class label. Unable to fit the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4013441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
