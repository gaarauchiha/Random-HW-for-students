{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3858ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.52\n",
      "accuracy_baseline: 0.50\n",
      "auc: 0.54\n",
      "auc_precision_recall: 0.53\n",
      "average_loss: 0.72\n",
      "global_step: 60.00\n",
      "label/mean: 0.50\n",
      "loss: 0.72\n",
      "precision: 0.52\n",
      "prediction/mean: 0.51\n",
      "recall: 0.55\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ASUSN5~1\\AppData\\Local\\Temp\\tmpaors_lad\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\ASUSN5~1\\\\AppData\\\\Local\\\\Temp\\\\tmpaors_lad', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\ASUSN5~1\\AppData\\Local\\Temp\\tmpaors_lad\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.7650104, step = 0\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 60...\n",
      "INFO:tensorflow:Saving checkpoints for 60 into C:\\Users\\ASUSN5~1\\AppData\\Local\\Temp\\tmpaors_lad\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 60...\n",
      "INFO:tensorflow:Loss for final step: 0.7727953.\n"
     ]
    }
   ],
   "source": [
    "'''import the required packages and read the file.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "\n",
    "random.seed( 10 )\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('input_file_2.csv', sep=',', index_col=0)\n",
    "data['issue_d'] = pd.to_datetime(data['issue_d'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df = data.loc[data['issue_d'] < data['issue_d'].quantile(0.75)]\n",
    "test_df = data.loc[data['issue_d'] >= data['issue_d'].quantile(0.75)]\n",
    "\n",
    "# Drop the 'issue_d' column\n",
    "train_df = train_df.drop('issue_d', axis=1)\n",
    "test_df = test_df.drop('issue_d', axis=1)\n",
    "\n",
    "# Define columns to scale\n",
    "all_cols = list(train_df.columns)\n",
    "all_cols.remove('charged_off')\n",
    "to_drop_categorical = ['home_ownership', 'verification_status', 'purpose', 'application_type']\n",
    "for i in to_drop_categorical:\n",
    "    all_cols.remove(i)\n",
    "\n",
    "# Fill null values by mean imputation\n",
    "train_df[all_cols] = train_df[all_cols].fillna(train_df[all_cols].mean())\n",
    "test_df[all_cols] = test_df[all_cols].fillna(train_df[all_cols].mean())\n",
    "\n",
    "# Scale values of numerical columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=False)\n",
    "train_df[all_cols] = scaler.fit_transform(train_df[all_cols])\n",
    "test_df[all_cols] = scaler.transform(test_df[all_cols])\n",
    "\n",
    "# Balance classes for training and testing\n",
    "train_dat_1s = train_df[train_df['charged_off'] == 1]\n",
    "train_dat_0s = train_df[train_df['charged_off'] == 0]\n",
    "keep_0s = train_dat_0s.sample(frac=train_dat_1s.shape[0]/train_dat_0s.shape[0])\n",
    "train_df = pd.concat([keep_0s,train_dat_1s],axis=0)\n",
    "\n",
    "test_dat_1s = test_df[test_df['charged_off'] == 1]\n",
    "test_dat_0s = test_df[test_df['charged_off'] == 0]\n",
    "keep_0s = test_dat_0s.sample(frac=test_dat_1s.shape[0]/test_dat_0s.shape[0])\n",
    "test_df = pd.concat([keep_0s,test_dat_1s],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def easy_input_function(df, label_key, num_epochs, shuffle, batch_size):\n",
    "  label = df[label_key]\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df),label))\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(10000)\n",
    "\n",
    "  ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "  return ds\n",
    "\n",
    "###################################################################\n",
    "import functools\n",
    "\n",
    "'''Define training and test input functions with their parameters.'''\n",
    "\n",
    "train_inpf = functools.partial(easy_input_function, train_df, label_key='charged_off',  num_epochs=5, shuffle=True, batch_size=20000)#300000 #230934\n",
    "test_inpf = functools.partial(easy_input_function, test_df, label_key='charged_off', num_epochs=1, shuffle=False, batch_size=200000) #200000\n",
    "###################################################################\n",
    "\n",
    "#DEFINE ALL NUMERIC COLUMNS\n",
    "\n",
    "loan_amnt = fc.numeric_column('loan_amnt')\n",
    "term = fc.numeric_column('term')\n",
    "installment = fc.numeric_column('installment')\n",
    "emp_length = fc.numeric_column('emp_length')\n",
    "dti = fc.numeric_column('dti')\n",
    "earliest_cr_line = fc.numeric_column('earliest_cr_line')\n",
    "open_acc = fc.numeric_column('open_acc')\n",
    "pub_rec = fc.numeric_column('pub_rec')\n",
    "revol_util = fc.numeric_column('revol_util')\n",
    "total_acc = fc.numeric_column('total_acc')\n",
    "mort_acc = fc.numeric_column('mort_acc')\n",
    "pub_rec_bankruptcies = fc.numeric_column('pub_rec_bankruptcies')\n",
    "log_annual_inc = fc.numeric_column('log_annual_inc')\n",
    "fico_score = fc.numeric_column('fico_score')\n",
    "log_revol_bal = fc.numeric_column('log_revol_bal')\n",
    "\n",
    "my_numeric_columns = [loan_amnt,\n",
    "term,\n",
    "installment,\n",
    "emp_length,\n",
    "dti,\n",
    "earliest_cr_line,\n",
    "open_acc,\n",
    "pub_rec,\n",
    "revol_util,\n",
    "total_acc,\n",
    "mort_acc,\n",
    "pub_rec_bankruptcies,\n",
    "log_annual_inc,\n",
    "fico_score,\n",
    "log_revol_bal]\n",
    "\n",
    "##############################################\n",
    "\n",
    "#RETRAIN MODEL ON ALL THESE CATEGORICAL COLUMNS AS WELL\n",
    "\n",
    "\n",
    "def metric_auc(labels, predictions):\n",
    "    return {\n",
    "        'auc_precision_recall': tf.metrics.auc(\n",
    "            labels=labels, predictions=predictions['logistic'], num_thresholds=200,\n",
    "            curve='PR', summation_method='careful_interpolation')\n",
    "    }\n",
    "\n",
    "def metric_recall_0(labels, predictions):\n",
    "    return {\n",
    "        'recall_0': tf.metrics.recall(\n",
    "            labels=labels, predictions=predictions['logistic'], name = '0')\n",
    "    }\n",
    "\n",
    "def metric_recall_1(labels, predictions):\n",
    "    return {\n",
    "        'recall_1': tf.metrics.recall(\n",
    "            labels=labels, predictions=predictions['logistic'], name = '1')\n",
    "    }\n",
    "\n",
    "########################################################\n",
    "\n",
    "#NOW FOR CATEGORICAL COLUMNS...\n",
    "\n",
    "print('Now encoding categorical columns')\n",
    "\n",
    "'''relationship = fc.categorical_column_with_vocabulary_list(\n",
    "    'relationship',\n",
    "    ['Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'])\n",
    "\n",
    "print(fc.input_layer(feature_batch, [age, fc.indicator_column(relationship)]))'''\n",
    "\n",
    "home_ownership = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'home_ownership', hash_bucket_size=100000)\n",
    "\n",
    "verification_status = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'verification_status', hash_bucket_size=100000)\n",
    "\n",
    "purpose = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'purpose', hash_bucket_size=100000)\n",
    "\n",
    "application_type = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'application_type', hash_bucket_size=100000)\n",
    "\n",
    "\n",
    "\n",
    "print('actual DNN Grid Search with non-linearity')\n",
    "\n",
    "for layer_1 in [5, 10, 15, 20, 30]:\n",
    "    for layer_2 in [1, 3, 5, 10]:\n",
    "\n",
    "        model_l2 = tf.estimator.DNNClassifier(\n",
    "            hidden_units=[layer_1, layer_2],\n",
    "            feature_columns=my_numeric_columns,\n",
    "            activation_fn=tf.nn.tanh,\n",
    "            dropout=0.2,\n",
    "            optimizer=\"Adam\")\n",
    "\n",
    "        #model_l2 = tf.contrib.estimator.add_metrics(model_l2, metric_auc)\n",
    "\n",
    "\n",
    "        model_l2.train(train_inpf)\n",
    "\n",
    "        print('TEST RESULTS ', layer_1, layer_2)\n",
    "\n",
    "        results = model_l2.evaluate(test_inpf)\n",
    "        clear_output()\n",
    "        for key in sorted(results):\n",
    "            print('%s: %0.2f' % (key, results[key]))\n",
    "\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eae2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
